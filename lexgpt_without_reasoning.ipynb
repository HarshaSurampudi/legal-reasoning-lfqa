{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e05321-2559-4cad-bccb-60f1c6141c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62651f8-6c29-44f7-81e0-3db72a0453d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A6000 (UUID: GPU-bdfc7745-2817-afa0-d7d6-cc6cf68cc74d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb10ab1-3844-4220-8eab-59a92997519e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed691c5f3d164202aa4c76a3faea7bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"patent/LexGPT-6B\",\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patent/LexGPT-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1fc7b0-0581-4cf4-bdc7-0b2dbf519c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1778062-c64e-40e3-ac84-aeb0a5f25197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206229d6-0a44-40ad-9eb9-c40d8e060856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7340032 || all params: 6058222816 || trainable%: 0.12115817167725645\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c2a358-9bd3-4083-a97b-b1ddc722a912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train_without_reasoning.csv\",\n",
    "    block_size=256,\n",
    ")\n",
    "\n",
    "val_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train_without_reasoning.csv\",\n",
    "    block_size=256,\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a611f76e-4717-4a35-adf6-68eb482815c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the parameters for fine-tuning\n",
    "lr = 1e-5\n",
    "end_lr = 2e-6\n",
    "num_train_epochs = 1\n",
    "warmup_steps = 100\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir='lexgpt-without-reasoning-results',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=warmup_steps,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.1,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "    ),\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd311555-ec52-4524-8f16-3fb942d6351c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/legal-lfqa/wandb/run-20230815_205329-3hw49pfx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/harshasurampudi/huggingface/runs/3hw49pfx\" target=\"_blank\">twilight-sun-22</a></strong> to <a href=\"https://wandb.ai/harshasurampudi/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='609' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [609/609 2:10:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.876600</td>\n",
       "      <td>8.815628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.691400</td>\n",
       "      <td>8.557896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>8.115500</td>\n",
       "      <td>8.117171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.843000</td>\n",
       "      <td>7.759498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>7.518200</td>\n",
       "      <td>7.463670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>7.313800</td>\n",
       "      <td>7.281152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>7.207800</td>\n",
       "      <td>7.165081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.120900</td>\n",
       "      <td>7.081820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>7.076900</td>\n",
       "      <td>7.021309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.020900</td>\n",
       "      <td>6.980078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>6.946900</td>\n",
       "      <td>6.954391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.963100</td>\n",
       "      <td>6.944281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.089904627525443 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.366542973531505 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 9.998366176521644 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.4426090082797773 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.4983613737022825 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.55208073485696 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 8.001060760536065 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.479601358668867 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.05551224173313 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.325921832381494 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 8.432844330417375 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.196378972266973 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.575539152989342 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 9.94727072970025 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.123056508765643 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.482753427474256 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.898569767747379 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 9.69384685192765 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.4823515123081514 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.138198622803441 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.762425769415882 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 9.837173465184465 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.4136114952187118 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.109160659967492 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 8.709165142460686 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.3282768249188672 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.8679696804928865 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 8.906402643810743 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.083505618698874 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 2.310160891914728 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 4.584653518993983 seconds), retrying request\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 429 encountered (Filestream rate limit exceeded, retrying in 9.806309521549712 seconds), retrying request\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5ec3ac-0669-4dfd-bd5d-cd3122ea7c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:831: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2883258c21254bcb9db19d23ae048d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/29.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/harsha28/lexgpt-lfqa-without-reasoning/commit/7caf66aa603c9d7f1b9731cf6a331d579d28b928', commit_message='lr 1e-5, 1 epoch', commit_description='', oid='7caf66aa603c9d7f1b9731cf6a331d579d28b928', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"harsha28/lexgpt-lfqa-without-reasoning\",\n",
    "                  use_auth_token=True,\n",
    "                  commit_message=\"lr 1e-5, 1 epoch\",\n",
    "                  private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0666e4-f85e-4c73-8abd-f7eddfff9347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1515b9ec28ee4114a8d069a1a54e6e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/adapter_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c7be444cb14db085ec9c679a5ea269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806a7bc887e44bf193154b2216621e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/29.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"harsha28/lexgpt-lfqa-without-reasoning\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patent/LexGPT-6B\")\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fa6162-a286-440b-aacd-dbdbf04ae791",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " context: Stat. § 14-32.1(a), does not make the definition an essential element of the crime pursuant to N.C. Gen. Stat. § 14-32.1(e). Therefore, we reject Defendant’s argument that it is not sufficient for the indictment to “merely state that the victim was ‘handicapped.’ ” Furthermore, the indictment provided Defendant with enough information to prepare a defense for the offense of felony assault on a handicapped person. See Leonard, _ N.C. App. at _, 711 S.E.2d at 873 (rejecting the defendant’s argument that the indictment was not sufficient because the indictment tracked the relevant language of the statute, listed “the essential elements of the offense[,]” and provided the defendant “with enough information to prepare a defense”); State v. Crisp, 126 N.C. App. 30, 36, 483 S.E.2d 462, 466 (<HOLDING>), appeal dismissed and disc. review denied, 346 <question> Is it necessary for the definition of the crime to be stated in the indictment according to N.C. Gen. Stat. § 14-32.1(a)? answer:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"context: Stat. § 14-32.1(a), does not make the definition an essential element of the crime pursuant to N.C. Gen. Stat. § 14-32.1(e). Therefore, we reject Defendant’s argument that it is not sufficient for the indictment to “merely state that the victim was ‘handicapped.’ ” Furthermore, the indictment provided Defendant with enough information to prepare a defense for the offense of felony assault on a handicapped person. See Leonard, _ N.C. App. at _, 711 S.E.2d at 873 (rejecting the defendant’s argument that the indictment was not sufficient because the indictment tracked the relevant language of the statute, listed “the essential elements of the offense[,]” and provided the defendant “with enough information to prepare a defense”); State v. Crisp, 126 N.C. App. 30, 36, 483 S.E.2d 462, 466 (<HOLDING>), appeal dismissed and disc. review denied, 346 <question> Is it necessary for the definition of the crime to be stated in the indictment according to N.C. Gen. Stat. § 14-32.1(a)? answer: \", return_tensors='pt')\n",
    "\n",
    "batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=1000)\n",
    "\n",
    "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cb12f-4243-4830-be5e-14dd3a165028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
